\chapter{Introducción}
% \chapter{Introduction}

\section{Definición del problema}
% \section{Problem definition}

El reconocimiento automático del habla (Automatic Speech Recognition ASR) es un campo de la Inteligencia Artificial estudiado desde los años 50 con el objetivo de que los ordenadores entendieran el lenguaje natural hablado humano. Las primeras aproximaciones se hicieron reconociendo dígitos \cite{Davis1952AutomaticDigits}, y en la actualidad tenemos reconocedores capaces de reconocer grandes vocabularios como Google Home \cite{Li2017}, Microsoft Cortana \cite{Xiong2017} y muchos otros asistentes personales.
% Automatic Speech recognition (ASR) is a subfield of Artificial Intelligence (AI) studied since the early fifties recognizing single digits \cite{Davis1952AutomaticDigits}, to large vocabulary recognizer included in Google Home \cite{Li2017}, Microsoft Cortana \cite{Xiong2017}, and many others Intelligent personal assistants.

Las cadenas ocultas de Markov (Hidden Markov Models HMM) \cite{RabinerARecognition,PaulTheRecognizer,Zue1989TheReport} y las redes neuronales artificiales (Artificial Neural Networks ANN) \cite{Waibel1989PhonemeNetworks,XuedongHuangAlexAceroHsiao-WuenHon201,Hwang} han mostrado grandes resultados en la tarea del reconocimiento automático del habla, pero grandes cantidades de datos son necesarias. El incremento de la cantidad de los datos aumenta la precisión de los modelos, medidos usualmente como la proporción de error en palabras (Word Error Rate WER), como muestró Roger K. Moore en su artículo para InterSpeech 2003
% Hidden Markov Models (HMM) \cite{RabinerARecognition,PaulTheRecognizer,Zue1989TheReport} and Artificial Neural Networks (ANN) \cite{Waibel1989PhonemeNetworks,XuedongHuangAlexAceroHsiao-WuenHon201,Hwang} have proven great results on ASR tasks, however, large amounts of data are required. Increasing  training data can improve the Word Error Rate (WER) as shown in Roger K. Moore InterSpeech 2003 Article \cite{MooreAListeners}.

Para el idioma inglés, corpus anotados y hablados de larga duración y gran vocabulario están disponibles y son usados para diversos estudios, los mas populares son FISHER \cite{CieriTheSpeech-to-Text} y Libri-Speech  \cite{PanayotovLIBRISPEECH:BOOKS}  los cuales cuentan con mas de 1000 horas de habla anotada, dando a los investigadores suficiente información para entrenar modelos con bajos WER \cite{HannunDeepRecognition}.
% For the English language, large vocabulary-long duration annotated speech corpora are available and used in several studies. Popular corpus like FISHER \cite{CieriTheSpeech-to-Text} and Libri-Speech  \cite{PanayotovLIBRISPEECH:BOOKS}  has over 1000 hours of annotated speech giving researches enough data to train largest models with WER \cite{HannunDeepRecognition}.

Por otro lado, para el idioma español, los recursos disponibles como CALLHOME \cite{CALLHOMESpa}, CALLFRIEND  \cite{CALLFRIENDSpa}, Voxforge \cite{Voxforge.org}, y CIEMPIESS \cite{Hernandez-MenaCIEMPIESS:Corpus} tienen menos de 100 horas cada uno, y estudios similares muestran WER significatibamente altos \cite{Hernandez-Mena2017AutomaticResources}.
% On the other hand, for the Spanish language corpora are considerable small where popular corpora like CALLHOME \cite{CALLHOMESpa}, CALLFRIEND  \cite{CALLFRIENDSpa}, Voxforge \cite{Voxforge.org}, and CIEMPIESS \cite{Hernandez-MenaCIEMPIESS:Corpus} had less than 100 hours each and where WER is considerably higher \cite{Hernandez-Mena2017AutomaticResources}

Por otro lado existen grandes bases de datos abiertas de audio libros para el idioma español, las cuales no tienen un nivel de anotación adecuado para realizar investigación en reconocimiento automático de voz, pero cuentan con las caracterísiticas necesarias para aumentar el nivel de anotación usando técnicas de alineamiento forzado (Forced Alignment FA). 
% There are open audio resources for the Spanish language, are not annotated or the level of annotation is not segmented enough. The process of define the time intervals where a unit of speech is spoken in a speech audio given the transcription is called Forced Alignment (FA)

El objetivo principal de este proyecto es crear un alineador forzado que anote una de estas bases de datos de licencia abierta para el lenguaje español.
% The main objective of this project is to create a FA that can annotates a large duration-open sourced Spanish speech corpora.

\section{Justificación}
% \section{Justification}

La precisión de un reconocedor automático incrementa con la cantidad de datos utilizados para el entrenamiento \cite{MooreAListeners}. Estos datos deben tener una anotación, es decir una relación entre señal de habla y representación. Las representaciones se hacen a nivel de expresión, palabra o fonema, siendo la segmentación a nivel de expresiones suficiente en la mayoría de los casos. La mayoría de investigación académica usa el idioma Inglés como línea base para sus investigaciones usando algunos de los corpus anotados mostrados en la tabla \ref{tab:english_corpora}, por otro lado, los recursos para el idioma Español se muestran en la tabla \ref{tab:spanish_corpora} donde se evidencia la diferencia entre los tamaños de los datos.

% The accuracy of Speech Recognizer models increases with more data to train \cite{MooreAListeners}, optimal data must be annotated and segmented in small amounts where phonetic level segmentation are ideal, but words and utterances segmentation are enough in most cases.  Most academic research uses the English language as the baseline for testing recognizers, having multiples corpora annotated at utterance level to validate their models as shown in table \ref{tab:english_corpora}, on the other hand, resources for the Spanish languages are limited having at much fewer resources to train models as shown in table \ref{tab:spanish_corpora}.

\input{introduction/table_corpora_english.tex}
\input{introduction/table_corpora_spanish.tex}

Aunque la brecha entre los lenguajes español e inglés es muy amplia, el lenguaje español no se considera un lenguaje con escasos recursos, pues existen múltiples corpus, bancos de árboles, datos anotados y transcritos, diccionarios y gramáticas formales \cite{CavarGlobalGORILLA}. Un gran recurso anotado disponible para el idioma español es la base de datos LibriVox  \cite{LibriVox}, la cual está basada en el proyecto Gutenberg \cite{gutenberg} y cuenta con mas de 400 audiolibros publicados bajo licencias abiertas. Un gran recurso creado por David Povey es llamado Libri Speech \cite{PanayotovLIBRISPEECH:BOOKS} el cual usa el proyecto Librvox con textos en idioma inglés como base y aumenta su nivel de anotación, dejando también esta anotación con licencia abierta.
% Although the gap between both languages, Spanish aren't considered an under resource language because it has corpora, treebanks, transcribed and annotated speech data, and just dictionaries and formal grammars  \cite{CavarGlobalGORILLA}. Speech annotated open resources are available for the Spanish language, and a big database is LibriVox \cite{LibriVox}, this is based on project Gutenberg \cite{gutenberg} and have more than 400 audiobooks published under public licenses. A large corpus created by David Povey and called Libri Speech \cite{PanayotovLIBRISPEECH:BOOKS} is based on Librivox project and also available with open licences.

Usando recursos de licencia abierta para el lenguaje español y mejorando su nivel de anotación permite que la comunidad entera de investigación de lenguaje hablado en español se beneficie, creando recursos de gran vocabulario y larga duración para posteriores investigaciones.
% Using open annotated resources for the Spanish language and improving its annotation level can benefit the whole speech research community creating an open source large vocabulary long duration corpora for future work

\section{Objetivos}
% \section{Objectives}

En esta sección, se presentan el objetivo general y los objetivos específicos de esta tesis.
% In this section, general objective and specific objectives  are presented.

\subsection{Objetivo general}
% \subsection{General objective}

Generar automáticamente un corpus para el lenguaje español de gran vocabulario y larga duración a partir de recursos existentes y usando alineadores forzados.
% To automatically annotate a large Spanish speech corpora using forced aligners

\subsection{Objetivos específicos}
% \subsection{Specific objectives}

\begin{itemize}
    \item Estudiar los algoritmos para alineadores forzados y sus implementaciones.
    % \item To study forced aligners algorithms and implementations
    \item Recolectar recursos y construir un corpus de prueba para medir el desempeño de alineadores forzados en Español.
    % \item To collect resources and design a test corpus to measure performance on Spanish aligners
    % \item To compare and extract relevant features of existing forced aligners
    \item Diseñar e implementar un alineador forzado que mejore el nivel de anotación de un corpus de gran vocabulario y larga duración.
    % \item To design and implement a forced aligner that annotates a large duration Spanish speech corpora
    \item Mejorar el nivel de anotación de un corpus de gran vocabulario y larga duración para el idioma español.
    % \item To annotate a large duration-open sourced Spanish speech corpora

\end{itemize}

% Exprese claramente y seleccione de forma adecuada el verbo en infinitivo que expresa que va a
% hacer en su propuesta.
% • El objetivo general debe permitir responder la formulación del problema y proponer una
% solución viable y factible al problema propuesto.
% • Los objetivos específicos deben aportar al objetivo general y en esta medida deben ser
% evaluables.
% • Recuerde los objetivos específicos deben ser claros y precisos. La selección del verbo adecuado
% para expresar el objetivo específico le permitirá estructurar bien la propuesta. Es distinto
% analizar, diseñar o modelar aunque puedan confundir al momento de implementar un sistema.
% • Recuerde que la propuesta cuando sea aceptada, los objetivos se convierten en los elementos
% que permiten evaluar el proyecto.


\section{Metodología}
% \section{Methodology}

Para lograr cada objetivo específico, se inicia con una exploración a la literatura académica de publicaciones relacionada con alineadores forzados y sus aplicaciones para mejorar el nivel de anotación de corpus hablados, estudiando los principios propuestos, implementación y relevancia en el presente.
% To accomplish each one specific objectives, we starts with a literature exploration of academic publications searching for existing forced aligner, its core principles, implementations, and relevance over the time.


Simultáneamente se recolectarán recursos de licencias abiertas anotados en un nivel apropiado para evaluar el desempeño de los alineadores forzados, diseñando un corpus de prueba fonéticamente balanceado para el idioma español.
% Simultaneously it will be collected a set of resources to measure the performance of existing forced aligners, designing a balanced phonetically segmented corpus for the Spanish language.

Usando conceptos clave de los alineadores mas precisos, se diseñará y construirá un alineador forzado optimizado para el idioma español el cual pueda mejorar el nivel de alineación de los recursos abiertos seleccionados.
% Using core concepts from the most accurate aligners, a new aligner will be designed and built which can annotates the selected corpora.

Se liberará bajo licencias abiertas el corpus generado dejando también el alineador abierto para futuras investigaciones.
% Annotating and open sourcing the annotation of the corpora for future work.

% La metodología debe presentar en forma organizada, cómo será alcanzado cada uno de los
% objetivos específicos. La metodología debe reflejar una estructura lógica para desarrollar el proyecto, deben estar detallados los procedimientos, técnicas, actividades y demás estrategias para alcanzar los objetivos. Debe mostrar de forma clara el proceso de recolección de
% información o de requisitos, así como en la organización, sistematización y análisis. De igual forma, las elecciones para el desarrollo del proyecto tales como el lenguaje de modelado o el lenguaje de programación. Si utiliza una metodología estándar descríbala claramente, utilice algunas referencias bibliográficas de la metodología utilizada.


\section{Resultados esperados}
% \section{Expected results}

Se espera obtener los siguientes resultados de esta tesis:
% With this thesis we expect the following results:

\begin{itemize}
    \item Un análisis detallado de los alineadores forzados existentes en idiomas inglés y español
    % \item A detailed analysis of forced aligners its performance on English and Spanish languages
    \item Un corpus de prueba en español para medir la precisión de un alineador forzado.
    % \item A test corpora in Spanish to measure the precision of forced aligners
    \item Un nuevo alineador forzado optimizado para el idioma español.
    % \item A newly forced aligner optimized for Spanish language
    \item Un corpus de gran vocabulario y larga duración anotado a nivel de declaración para el idioma español.
    % \item An annotation of a large duration-open sourced Spanish speech corpora
\end{itemize}

% para ello identifique por cada una de las
% actividades cuáles son los resultados esperados, ellos deben ser tangibles y verificables, en el
% caso de que esto no sea posible incluya un informe de la actividad. En los proyectos de
% Ingeniería de Sistemas comúnmente los resultados son: modelos, códigos, pruebas realizadas, etc .


\section{Marco teórico}
% \section{Theoretical Framework}

% \subsection{Architecture of an ASR}

% Traditional architectures of an ASR are composed by 4 elements as illustrated on Figure \ref{fig:architecture}: a Signal Processing and Feature Extraction (FE), an Acoustic Model (AM) a Language Model (LM) and a Hypothesis Search.

% \begin{figure}
% \centering
% \begin{tikzpicture}[
% squarednode/.style={rectangle, draw=black!60, fill=white!5, very thick, minimum size=5mm},
% ]
% %Nodes
% \node[squarednode]      (am)                              {Acoustic model};
% \node[squarednode]        (fe)       [above=of am] {Feature Extraction};
% \node[squarednode]        (wave)       [above=of fe] {Audio Signal};
% \node[squarednode]      (hypothesis)       [right=2cm of am] {Hypothesis Search};
% \node[squarednode]        (lm)       [below=of hypothesis] {Language Model};
% \node[squarednode]        (result)       [above=of hypothesis] {Recognition Result};
 
% %Lines
% \draw[->] (wave.south) -- (fe.north);
% \draw[->] (fe.south) -- node{Feature}(am.north);
% \draw[->] (am.east) -- node{AM Score}(hypothesis.west);
% \draw[->] (lm.north) -- node{LM Score}(hypothesis.south);
% \draw[->] (hypothesis.north) -- (result.south);
% \end{tikzpicture}

% \caption{Classic architecture of an ASR \cite{Yu_2014_1}}
% \label{fig:architecture}
% \end{figure}

% The Feature Extraction (FE) process is usually called Front End because is the first step of Speech Recognition, its labor is to enhance the speech, removing distortions noise and transforming the wave in a representation that can be used from the Acoustic Model (AM), the AM estimates the probability for the transformed audio input given a known context and returns a score for the variable-length, the Language Model (LM) estimates the probability of a hypothesized word sequence; this approach uses N-grams with a very large collection of text, finally the Hypothesis Search merges the output of the AM and the LM. 
% Forced Alignment (FA) can be used inside the AM before begin the model training as used for CMU Sphinx \cite{Lee1990AnSystem}

Alineamiento Forzado (Forced Alignment FA) es un conjunto de técnicas para identificar los intervalos de tiempo entre los cuales una unidad de habla ocurre.
% Forced Alignment is a set of techniques to identify time intervals where speech units occur in transcribed speech. 

Para el análisis del habla, una unidad de habla es el segmento más pequeño en el cual está dividida la anotación de una señal; esta puede ser a nivel de declaración, la cual se define como el espacio de sonidos entre dos silencios, palabras o fonemas. Para el reconocimiento automático de la voz (Automatic Speech Recognition ASR), se usa generalmente una declaración para el entrenamiento y prueba de los algoritmos, sin embargo para el proceso de alineación forzada, se usa el nivel de fonemas.
% For speech analysis, a speech unit can be utterances, the speech audio between two silences, words or phonemes. Automatic Speech Recognition (ASR) generally uses utterance for its algorithms, however FA require a more fine grained annotation, usually at phoneme level.

Desde 1888, el Alfabeto Fonético Internacional (International Phonetic Alphabet IPA) es el métidi estándar para representar y clasificar fonemas, indidicando con un alto nivel de precisión las variaciones en la manera de articulación o la localización vocal de las modulaciones. Pequeñas variaciones en los fonemas son llamadas alófonos y pueden aumentar el nivel de anotación  \cite{IPAAlphabet}, pero en la práctica no son usados.
% Since 1888 the International Phonetic Alphabet (IPA) is the standard method to represent and classify phonemes, indicating with a high level of precision small variations of the manner of articulation or location of vocal tract modulations. Little variations on phonemes are called allophones and can improve the level of annotation \cite{IPAAlphabet}, but is not used in practice.

Las vocales son caracterizadas por la posición de la lengua y el tamaño de la abertura de la boca, teniendo diferentes representaciones, que se pueden observar en la tabla \ref{tab:ipa_table_vowels}. 
% Vowels are characterized by the position of the tongue and the wideness of the mouth having different representations variating the position as shown in Table \ref{tab:ipa_table_vowels}. 

Las consonantes están clasificadas en dos segmentos: las pulmónicas, que requieren aire saliendo de los pulmónes y las no pulmónicas, donde no sale aire. En ambas las modificaciones en el tracto bucal definen el sonido. Las consonantes están divididas en dentales, coronales, dorsales y laríngeas, las cuales a su vez están subdivididas en otros sub segmentos. También la manera de articular define el sonido producido, siendo las maneras de articular: plosivas, nasales, oclusivas, fricativas, aproximantes, eyectivas o implosivas.

Vea la clasificación de las consonantes en las tablas \ref{tab:ipa_table_pulmonic_consonants} y \ref{tab:ipa_table_non_pulmonic_consonants}.
% Consonants are classified in pulmonic and non-pulmonic, where modifications on superior vocal tract define the sound. All consonants are segmented in labial which subdivides in bi-labial labio-dental and labio-lingual; coronal divided into dental, alveolar, post-alveolar and retroflex; dorsal divided into palatal, velar, uvular; and laryngeal, divided in  pharyngeal and glottal; each classification can have a relationship with an articulation manner which is plosive, nasal, thrill, tap, flap, fricative, approximant, ejective, click or implosives. 

La representación del Alfabeto Fonético Internacional para las vocales está mostrado en la tabla \ref{tab:ipa_table_vowels}, donde se clasifican los sonidos por su centro silábico y la apertura del tracto bucal.
% IPA representation for vowels are mentioned in table \ref{tab:ipa_table_vowels}, pulmonic consonants are mentioned on table \ref{tab:ipa_table_pulmonic_consonants} and non-pulmonic consonants are mentiones on table \ref{tab:ipa_table_non_pulmonic_consonants}


\input{introduction/table_ipa_pulmonic_consonants.tex}
\input{introduction/table_ipa_non_pulmonic_consonants.tex}
\input{introduction/table_ipa_vowels.tex}

Each language uses a subset of phonemes to define its own pronunciation.

Every language's word has only one phonetic representation and the relation between each word and its phonetic representation are called a phonetic dictionary. Besides the phonetic representation of each phoneme other features are needed to represent speech phenomenons like silence, mumbling and other inaudible sounds that can be perceived in a speech recording.

Having the transcription of a speech audio the expected output from a FA is a list intervals where phonemes occurs.

After understanding the phonological principles and its computational representation, an audio computational representation is also required.

To digitally represent an analog speech audio a widely accepted way is to use Pulse Code Modulation (PCM) where a transducer senses wave corresponding to the speech on a uniform time interval and translated it in a digital scale. The quality of the digital representation depends on the sampling rate (quantity of measurements by second) and the bit depth (possible digital values to represent each sample). With this representation, an audio is just a list of values in a range that varies representing the pressure applied to a transducer. This format is usually represented with a .pcm extension, but other formats of raw audio exists, like .wav, where the raw audio is stored after a set of headers that defines sampling rate, bit depth, endianness and number of channels.

Raw representation of audio is useful to reconstruct easily the analog signal, but to understand the relation between audio segments and phonetic information new ways to represent audio needs to be define.

To have a more uniform way to represent each audio, short time-frequency analysis are used and the technique consists in split the audio in fixed length chunks and each chunk with a defined offset usually less than the chunk length itself, and extract frequency features of each chunk using transforms like Fourier or Wavelet, this phase is usually called Feature Extraction and techniques like Mel Frequency Cepstral Coefficients (MFCC)\cite{Davis1980ComparisonSentences}, Perceptual Linear Prediction (PLP) \cite{Hermansky1990PerceptualSpeech}, Linear frequency cepstral coefficients LFCC \cite{Davis1980ComparisonSentences}, Wavelet-packet features (WPF) \cite{Farooq2001MelRecognition}, Subband-based cepstral parameters (SBC) \cite{Sarikaya98waveletpacket}, Mixed wavelet packet advanced combinational encoder (MWP-ACE) \cite{NogueiraWaveletImplants}.

With this new representation a model to identify intervals and boundaries between phonemes are clear, because similar 

To create a model to align speech three major techniques are used, the first is called Dynamic Time Warping (DTW) \cite{Sakoe1978DynamicRecognition} where an artificial wave with the phonetic representation of the annotation is created and then an alignment algorithm minimize the difference using a distance matrix. Another widely used approach is to use HMM where a probabilistic model is created to represent the transition between phonemes and then use this model to align existing waves \cite{RabinerARecognition}. The last approach is ANN using a normalization model to homogenize speech in sequences of vectors and create a model that maximize the prediction of a new vector \cite{Deng2012}.

For DTW the main idea es to eliminate the fluctuation of a particular speech creating a time normalization where phonemes occur in a recurrent time interval. To achieve this normalization, dynamic programming algorithms are proposed to generate the maximum coincidence between two speech patterns. Time speech transformations are usually made in both signals, this is called a symmetric approach, and a function that minimizes the distance between both signals is the expected output. 

Generating an artificial wave with the phonetic representation of real speech, where the algorithm know a priori the intervals where each phoneme occurs and minimizing the distance between those two waves, a reconstruction of the real speech wave can be perform using the opposite sequence that minimize the distance between the waves.

Aligning a wave using HMMs requires a preexisting model that defines the hidden states of the markov models. This estimation usually uses a Baum-Welch algorithm, where a set of values are proposed for the hidden layer usually randomly and maximized using an Expectation-Maximization algorithm. This model then allows to use the Viterbi algorithm to find a maximum sequence of probability of hidden states for a new sequence, where the sequence is a speech vector and the hidden state represents the phonemes.

ANN approaches uses vectorization to homogenize speech data and generates a network topology that minimize the error of a new vector.


\section{State of the Art}

\subsection{A brief history}

ASR can be defined as the process where computers can interpret speech signals and produce the corresponding text representation. This problem has been addressed since the early fifties and has evolved from analog filters where arrays of specific hardware extract information from the acoustic signal evaluating specific frequencies, compare from a large data bank and output a result  \cite{Davis1952AutomaticDigits,Olson1957PhoneticTypewriter}. This first approach works with high accuracy but was speaker dependent, which means only can recognize words from a single speaker. 

More sophisticated approaches were produce in the sixties and seventies where software filters and normalization let researchers work with Digital Signal Processing (DSP) and dynamic programming \cite{Velichko1970AutomaticWords,Sakoe1978DynamicRecognition,Itakura1975MinimumRecognition}, with those approaches speaker independent isolated word recognition with a high accuracy over a small controlled language.

Increasing the vocabulary size was a problem addressed in the eighties and nineties where approaches like Hidden Markov Models (HMM) \cite{RabinerARecognition} and Artificial Neural Networks (ANN) \cite{Waibel1989PhonemeNetworks} won popularity as methods to treat large vocabularies. A result of this popularity many software frameworks where develop like SPHINX by Carnegie Mellon University (CMU) \cite{Lee1990AnSystem}, BYBLOS \cite{ChowBYBLOS:System}, the Lincoln Robust Speech Recognizer \cite{PaulTheRecognizer} and the MIT Summit Speech Recognition System \cite{Zue1989TheReport}. Many of these systems use an HMM approach mixed with a Gaussian Mixture Models using Senones: a sub-phonetic unit which allows each phone sub model to share and cluster its state\cite{Hwang}.

All these framework shows great results on noise reduced environments, introducing a next challenge: improve accuracy on noisy channels, which starts the concept of Robust Speech Recognition \cite{SieglerOnSystems,MirghaforiTowardsASR}. Which includes research in reducing bandwidth of the sound perception and create a modulation spectrogram to reduce reverberation and noise perception \cite{Kingsbury1998RobustSpectrogram}, segment the audio wave and identify unreliable segments using marginalization and state-based data imputation combined with energy bounds which improved performance on noisy channels \cite{Cooke2001RobustData} and new hybrid approaches to improve performance combining existing HMM with ANN \cite{BourlardABands} to mention some relevant areas.

Introduction of Graphical Unit Processors (GPU) to accelerate machine learning algorithms for Image Classification \cite{KrizhevskyImageNetNetworks} also impact ASR, giving a new a rise of hybrid techniques using Context-Dependent Deep Neural Network Hidden Markov Model (CD-DNN-HMM) \cite{Yu_2014_1,Xiong2017} and also new frameworks based on Deep Learning techniques \cite{Povey_ASRU2011,1401.6984}.

%  El estado del arte, es una revisión de literatura (Internet, libros, revistas,etc) que permite identificar que otros trabajos similares hay en el área, o cuál es el borde del conocimiento que nos permite realizar la propuesta. Esta sección no debe superar una hoja. Tome lo desarrollado previamente y haga una sintesis.

\subsection{Existing tools}

Open source forced aligners uses theoretical research to create tools to materialize alignment on existing resources. Based on previously mentioned categorization the existing aligners are grouped in the categories: Dynamic Time Warping, Hidden Markov Models and Artifical Neural Networks.

\subsubsection{Dynamic Time Warping}

For DTW the main idea is to generate an artificial speech using Grapheme to Phoneme  software and then align the input speech with the artificially generated wave
\textbf{Aeneas}
Aeneas \cite{aeneas} uses espeak \cite{espeak} to generate the base generates speech wave. Then align the input word using dynamic programming.

\subsubsection{Hidden Markov Models}

Work with HMM uses a basic set of algorithms where the Viterbi algorithm \cite{Forney1973TheAlgorithm} and the Baum Welch Algorithm. These set of algorithms are implemented for ASR in frameworks and toolkits like HTK \cite{Young1994ThePhilosophy}, Julius \cite{LeeEurospeechEngine} and CMU Sphinx \cite{Lee1990AnSystem}.

\textbf{MAUS}
The Munich Automatic Segmentation \cite{WesenickAPPLYINGPRONUNCIATION} is a build on top of HTK and BALLON a Grapheme to Phoneme Suite created by Uwe Reichel and uses a hybrid approach between DTW and HMM

\textbf{SPPAS}
SPPAS \cite{Bigi2016ASPPAS} is a suite for automated for automated annotation and speech analysis created by Brigitte Bigi at Laboratoire Parole et Langage in France and uses Julius for HMM processing and Viterbi Algorithm

\textbf{Prosodylab Aligner}
Prosodylab Aligner \cite{Gorman2011Prosodylab-aligner:Speech} was created at Prosody Lab in Mcgill University by Kyle Gorman and are composed by a set of tools of scripts to create alignment using HTK as backend using mono-phones to train its models

\subsubsection{Artificial Neural Networks}

For ANN the Kaldi toolkit \cite{Povey_ASRU2011} is used to improve development times.   

\textbf{Gentle Forced Aligner}
Gentle \cite{gentle} is a FA build on top of Kaldi. The main approach is to use a web server to evaluate an input speech. The model uses dynamic programming to align audio using a bigram model

\textbf{The Montreal Forced Aligner}
The Montreal Forced Aligner \cite{McAuliffe2017MontrealKaldi} was created at Prosody Lab in Mcgill University as the evolution for Prosodylab Aligner. It uses Kaldi as backend and Globalphone to create a triphone model that improves performance comparing Prosodylab Aligner

% \section{Costs structure}